# Profile

Senior engineer focusing on backend, Big Data, and network services; with a
vast experience in distributed systems and cloud architectures. I am most
comfortable working in Unix environments, using a combination of Java, Perl,
and Python.

I like new challenges and learning, in a tech-friendly environment, and am
currently looking for a role where I can have a positive and meaningful
impact on people’s lives.


# Experience

## Founder and CTO at owlr — Oct 2013-present

In 2013 I co-founded Owlr with a couple of friends, with the goal of taming
the Internet of Things for consumers. I defined and implemented most of our
backend platform, which placed great emphasis on user privacy and took a
Zero Knowledge approach to user data, through heavy use of public key
cryptography. The whole platform runs on Amazon Web Services and was written
in Java using a services oriented architecture, with Dropwizard being used
extensively as the framework of choice to write services. It also uses AWS
provided services as much as possible, as we wanted to avoid management
overheads. Other than EC2, these include S3, RDS, SQS, SNS, Auto Scaling,
VPC, and ELB; with monitoring and alerts being provided by CloudWatch
integrated with third-party alert and monitoring solutions, and provisioning
handled by OpsWorks, CloudFormation, and Elastic Beanstalk.

## Big Data engineer at PeerIndex — Feb 2013-Mar 2013 (contract)

Part of PeerIndex’s DataTeam, in charge of a new solution to analyse and
classify tweets and user information coming from the Twitter firehose. The
solution encompasses data ingestion (both in-house data and a third-party
feed), real-time processing using Storm, MapReduce processing, and storage.
All the infrastructure is AWS-based, using Elastic MapReduce for Hadoop
clusters and a static cluster for Storm. The real-time processing was done in
Java, with the MapReduce jobs being written in Java and Hive. The solution
included a Python framework for describing and running complete job flows in
Elastic MapReduce.

## Big Data engineer at News International — Sep 2012-Feb 2013 (contract)

Part of the News International Customer Insights Program, is part of the team
designing and developing NI’s new analytics solution, which will allow the
company to analyse and classify customer behaviour across its web properties
(UK newspapers, US newspapers and television, and Australian newspapers) and
other online interactions, like social media. The solution encompasses data
ingestion (both in-house data and third-party feeds, like Datasift),MapReduce
processing, and storage (raw data is stored both in HDFS and Amazon S3,
granular processed data is stored in HBase). This work is almost completely
done in Java, with some parts written in Pig, Hive, and Python (notably the
web services providing access to the data warehouse via an API to anyone in
the company); using permanent clusters (with Cloudera Hadoop) on Amazon EC2.

## Big Data engineer at Channel 4 — Mar 2012-Sep 2012 (contract)

Joined Channel 4 to bring a PoC Hadoop analytics solution into production.
Designed and developed the full solution, from storage planning to data-
processing MapReduce jobs (in Java, Python, and Pig), and including a web
interface for analysts to manage their own clusters and run Hive queries. The
solution is built on top of Amazon Web Services, and provides the Strategy
team information about viewer habits and segmentation based on several
criteria. The stack involved included everything from event ingestion to
making results available to analysts in both fine-grained and aggregate
forms, and building RDBMS tables and Redis stores to support real-time
decision making by other services.

## Big Data engineer at Playfish - Mar 2009-Feb 2012

As part of the Analytics team, designed and implemented the company’s
Analytics and Big Data strategy. The team is responsible for collecting and
processing the events generated by all the company’s services (approximately
5 billion events a day, roughly 1TB of data), in order to produce data which
helps product managers and game designers make decisions based on customer
behaviour and sales results. All the Hadoop and Hive MapReduce jobs are ran
in Amazon Elastic MapReduce, with input data and results being stored in
Amazon S3, aggregate tables living both in Hive data warehouses for direct
consumption by data analysts and loaded into a traditional star-based data
warehouse on Vertica clusters for OLAP and feeding the BI tool used by end
users (MicroStrategy).

## Senior Perl developer at News Now Publishing - Aug 2008-Jan 2009

Member of the development team responsible for NewsNow’s web site and support
infrastructure. This was a high profile, high volume site (approximately 100
million page views/month), aggregating news stories from tens of thousands of
sources. Technologies used included OO Perl (using both CPAN and an extensive
library of modules developed in-house), job management systems (in-house),
load balancing, templating systems (in-house), Linux, MySQL, Amazon S3,
Amazon EC2, and Apache.

## Senior developer at Sky Broadband (formerly Easynet) - Jun 2006-Jul 2008

In the Network Applications Team, in charge of design and development in the
following areas: AAA (RADIUS & interfacing with British Telecom’s network)
and monitoring (bridge between legacy platforms and the new ones). Previous
work included the migration of the e-mail platform to Google infrastructure,
with the development of several tools to monitor and report on the
performance of the various components involved (e-mail, LDAP, RADIUS,
MySQL). Development of tools for defect tracking. Interfacing between the
e-mail platform and Sky’s CRM and provisioning platforms, in order to
guarantee data integrity and synchronicity between them. Design, development
and deployment of Sky Broadband’s first email platform (SMTP, POP and IMAP).
Also in the Network Applications team, dealt with interviewing and selecting
candidates, as well as mentoring new hires and doing in-house training on
development methodologies and best practices. Selected and recommended
external training and conferences.


# Education

University of Minho, Braga, Portugal — Systems Engineering and Informatics,
                                       Integrated Master’s

Languages: English (fluent), Portuguese (mother tongue),
           Spanish (conversational), French (conversational).


# Skills

## Software development

Perl, Java, Python, and C; TDD and OO Design methodologies. Good knowledge of
Objective-C, Javascript, and Ruby. Frequent experimentation with new
languages (currently, Go, Swift, and Scala).

Participation in several Open Source projects, code available on GitHub:
    https://github.com/pfig

## Databases

MySQL, Postgres, Oracle, and Amazon RedShift; as well as several key-value
datastores and NoSQL databases (Redis, Memcache, DynamoDB, MongoDB).

## Big Data

Hadoop ecosystem, Storm, and Amazon Elastic MapReduce. Cloudera Certified
Hadoop Developer.

## Infrastructure

Amazon Web Services, Vagrant, and Docker; automated deployments and
continuous integration.

## Hobbies

Photography, music composition, reading, and London.
